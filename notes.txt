Ways to be smarter about this:
  when should we re-scrape?
  scrape submission urls until we find one we already know, then stop
  occasionally scrape for new galleries

  documents:
    user: accounts[{type,user}], openid, name
    thing:  users{} set index, account{type,user}
            thumb url, source url, title, something age-related to sort by.  Maybe ratings

    whoops, looks like we need to key things with users, not accounts, sadly.  Lets refactor
    
    
    algorithm:
      when a user changes their account settings...
      they may have added or removed user accounts
      get a list of those added and removed
      
      
      do not download a thumb again if we already have it
      
    aw fuckit, this is too complicated.  Lets just paginate with javascript.

  issues:
    when you first log in, the last thing the view does is get_you, 
    meaning you may not have existed for the other queries on that page.
    Hmm, what's a better way to handle get_you?


  todo:
    make views permanent
    pagination with javascript
    deploy it :D
    icon -- choose an account, or put in an email address to get your gravatar.  upload?  mebbe
    make it look good -
    unique slugified names for urls -
      blank users just don't get to be in urls -
      Still, changing your name should enforce uniqueness unless it's blank -
    user settings page -
    background job to grab your stuff -
    user abstraction?  openid authentication! -
    types in the database (a user is not a post) -
    make it work with piclens
    fetch existing records from couch before scraping, so we know when we hid something we already have - CHECK
    cache the thumbnails myself with s3 - CHECK
  
  ethics page:
    mashup commandments:
      drive traffic back to the sites you scrape
      don't leech bandwidth (cached with s3)
    
    faq:
      It's illegal for you to scrape images from my site!
      Tell that to google images first
      
    
    
  ideas: (optional)
    show linked pages in an iframe with next/prev links, ratings, faves
    javascript page-mangling to make linked pages look better
    cache a hash of the thumbnail so we can tell if it's updated?  Or maybe its url would change...
    
    
  external dependencies:
    couchdb connection, couch.py, httplib2, simplejson
    