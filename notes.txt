Ways to be smarter about this:
  when should we re-scrape?
  scrape submission urls until we find one we already know, then stop
  occasionally scrape for new galleries

  documents:
    user: accounts[{type,user}], openid, name
    thing:  users{} set index, account{type,user}
            thumb url, source url, title, something age-related to sort by.  Maybe ratings

    whoops, looks like we need to key things with users, not accounts, sadly.  Lets refactor
    
    
    algorithm:
      when a user changes their account settings...
      they may have added or removed user accounts
      get a list of those added and removed
      
      
      do not download a thumb again if we already have it
      
    aw fuckit, this is too complicated.  Lets just paginate with javascript.

  issues:
    when you first log in, the last thing the view does is get_you, 
    meaning you may not have existed for the other queries on that page.
    Hmm, what's a better way to handle get_you?


  todo:
    put comments on stuff page
    test in internet explorer and safari
    deploy it with lighty+fastcgi like reddit
      needs couchdb (get an ebuild?), various python libs (easy_install)

    make views permanent - 
    ghetto ass pagination -
    refactor css into sections (corners, colors, etc) -
    icon = your latest scraped thumb -
    make it look good -
    unique slugified names for urls -
      blank users just don't get to be in urls -
      Still, changing your name should enforce uniqueness unless it's blank -
    user settings page -
    background job to grab your stuff -
    user abstraction?  openid authentication! -
    types in the database (a user is not a post) -
    make it work with piclens
    fetch existing records from couch before scraping, so we know when we hid something we already have - CHECK
    cache the thumbnails myself with s3 - CHECK
  
  ethics page:
    mashup commandments:
      drive traffic back to the sites you scrape
      don't leech bandwidth (cached with s3)
    
    faq:
      It's illegal for you to scrape images from my site!
      Tell that to google images first
      
    
    
  ideas: (optional)
    show linked pages in an iframe with next/prev links, ratings, faves
    javascript page-mangling to make linked pages look better
    cache a hash of the thumbnail so we can tell if it's updated?  Or maybe its url would change...
    pagination with javascript
      calculate available screen space and fills it with <a>s.
      pagination start algorithm: top left? maybe something more complex later
      slider for icon size, starts at 120px
      slider to scrub to a particular page?
        have it wait a moment so we don't waste bandwidth on the wrong page of thumbs
    
    
  external dependencies:
    couchdb connection, couch.py, httplib2, simplejson
    